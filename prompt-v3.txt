# Submodular Optimization for Diverse Query Generation

## Understanding Submodularity: The Mathematics of Diminishing Returns

Before trying to solve the problem, let me explain the term submodularity and submodular function to the readers. They may sound unfamiliar to many, but you may well have heard of the idea of "diminishing returns" - well, submodularity is the mathematical representation of that.

Consider placing Wi-Fi routers to provide internet coverage in a large building. The first router you install gives tremendous value - it covers a significant area that previously had no coverage. The second router adds substantial value too, but some of its coverage area overlaps with the first router, so the marginal benefit is less than the first. As you continue adding routers, each additional router covers less and less new area because most spaces are already covered by existing routers. Eventually, the 10th router might provide very little additional coverage since the building is already well-covered.

This intuition captures the essence of submodularity. Mathematically, a set function $f: 2^V \rightarrow \mathbb{R}$ is **submodular** if for all $A \subseteq B \subseteq V$ and any element $v \notin B$:

$$f(A \cup \{v\}) - f(A) \geq f(B \cup \{v\}) - f(B)$$

In plain English: adding an element to a smaller set gives at least as much benefit as adding the same element to a larger set that contains the smaller set.

## Problem Formulation

Now let's apply this concept to our query generation problem. Given an original input $q_0$, a set of candidate queries $V=\{q_1, q_2, \cdots, q_n\}$ generated by an LLM, we want to select a subset $X\subseteq V$ of $k$ queries that maximizes coverage while minimizing redundancy.

The key insight is that query selection exhibits natural diminishing returns:
- The first query we select covers entirely new semantic space
- The second query should cover different aspects, but some overlap is inevitable  
- As we add more queries, each additional query covers less and less new ground

## Embedding-Based Submodular Function Design

Let $\mathbf{e}_i \in \mathbb{R}^d$ be the embedding vector for query $q_i$, obtained using a sentence embedding model (e.g., jina-embeddings-v3). There are two main approaches to design our objective function:

### Approach 1: Facility Location (Coverage-Based)

$f_{\text{coverage}}(X) = \sum_{j=1}^{n} \max\left(\alpha \cdot \text{sim}(\mathbf{e}_0, \mathbf{e}_j), \max_{q_i \in X} \text{sim}(\mathbf{e}_j, \mathbf{e}_i)\right)$

This function measures how well the selected set $X$ "covers" all candidate queries, where:
- $\text{sim}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{|\mathbf{u}| |\mathbf{v}|}$ is the cosine similarity
- $\alpha \cdot \text{sim}(\mathbf{e}_0, \mathbf{e}_j)$ ensures relevance to the original query
- $\max_{q_i \in X} \text{sim}(\mathbf{e}_j, \mathbf{e}_i)$ measures coverage of candidate $j$ by selected set $X$

**Important caveat**: This function only *implicitly* encourages diversity. It doesn't explicitly penalize similarity within the selected set $X$. Diversity emerges because selecting similar queries provides diminishing coverage returns.

### Approach 2: Explicit Coverage + Diversity

For more direct control over diversity, we can combine coverage and an explicit diversity term:

$f(X) = \lambda \cdot f_{\text{coverage}}(X) + (1-\lambda) \cdot f_{\text{diversity}}(X)$

where the diversity component can be formulated as:

$f_{\text{diversity}}(X) = \sum_{q_i \in X} \sum_{q_j \in V \setminus X} \text{sim}(\mathbf{e}_i, \mathbf{e}_j)$

This diversity term measures the total similarity between selected queries and unselected queries - it's maximized when we select queries that are different from the remaining candidates (a form of graph cut function).

**Submodularity check**: Both formulations maintain submodularity. The facility location function is a well-known submodular function, and the graph cut diversity term is also submodular when measuring connections between selected and unselected sets.

## Why These Functions are Submodular

**Facility Location Function**: Exhibits submodularity because of the max operation. When we add a new query $q$ to our selected set, each candidate query $j$ gets covered by the "best" query in our set (the one with highest similarity). Adding $q$ to a smaller set $A$ is more likely to improve the coverage of various candidates than adding it to a larger set $B \supseteq A$ where many candidates are already well-covered.

**Graph Cut Diversity Function**: The diversity term $\sum_{q_i \in X} \sum_{q_j \in V \setminus X} \text{sim}(\mathbf{e}_i, \mathbf{e}_j)$ is submodular because it measures the "cut" between selected and unselected sets. Adding a new query to a smaller selected set creates more new connections to unselected queries than adding it to a larger selected set.

**Key insight**: The facility location approach relies on *implicit* diversity through coverage competition, while the explicit approach directly measures and optimizes for diversity. Both are valid, but the explicit approach gives you more direct control over the relevance-diversity tradeoff.

## Intuitive Understanding

Let me illustrate both approaches with a concrete example. Suppose our original query is "machine learning optimization" and we're selecting from these candidates:

1. "gradient descent algorithms"  
2. "neural network training"
3. "stochastic optimization methods" 
4. "backpropagation techniques"
5. "optimization for deep learning"

**Facility Location (Implicit Diversity)**:
- **First selection**: "gradient descent algorithms" covers fundamental optimization concepts
- **Second selection**: "neural network training" covers application domain with some overlap
- **Third selection**: "backpropagation techniques" provides less marginal coverage benefit since it's mostly in the intersection of optimization + neural networks

The diversity emerges naturally because similar queries compete to cover the same semantic regions.

**Explicit Coverage + Diversity**:
- The coverage term works as above
- The diversity term explicitly rewards selecting "stochastic optimization methods" over "backpropagation techniques" because it has higher similarity to unselected candidates (indicating it covers different semantic space)

**Which approach to use?**
- **Facility location**: Simpler, well-studied, good default choice
- **Explicit diversity**: More control over relevance-diversity balance, better when you need guaranteed diversity within selected set

You're absolutely right to point out that the facility location approach doesn't directly measure diversity *within* the selected queries - it only ensures good coverage of the candidate space. The explicit approach gives you that direct control.

## Optimization Algorithm

Since our function is submodular, we can use the greedy algorithm which provides a $(1-1/e) \approx 0.63$ approximation guarantee:

$$\max_{X \subseteq V} f(X) \quad \text{subject to} \quad |X| \leq k$$

```python
def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):
    """
    Greedy algorithm for submodular query selection
    
    Args:
        candidates: List of candidate query strings
        embeddings: Matrix of query embeddings (n x d)
        original_embedding: Embedding of original query (d,)
        k: Number of queries to select
        alpha: Relevance weight parameter
    """
    n = len(candidates)
    selected = []
    remaining = set(range(n))
    
    # Precompute relevance scores
    relevance_scores = cosine_similarity(original_embedding, embeddings)
    
    for _ in range(k):
        best_gain = -float('inf')
        best_query = None
        
        for i in remaining:
            # Calculate marginal gain of adding query i
            gain = compute_marginal_gain(i, selected, embeddings, 
                                       relevance_scores, alpha)
            if gain > best_gain:
                best_gain = gain
                best_query = i
        
        if best_query is not None:
            selected.append(best_query)
            remaining.remove(best_query)
    
    return [candidates[i] for i in selected]

def compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):
    """Compute marginal gain of adding new_idx to selected set"""
    if not selected:
        # First query: gain is sum of all relevance scores
        return sum(max(alpha * relevance_scores[j], 
                      cosine_similarity(embeddings[new_idx], embeddings[j]))
                  for j in range(len(embeddings)))
    
    # Compute current coverage
    current_coverage = [
        max([alpha * relevance_scores[j]] + 
            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])
        for j in range(len(embeddings))
    ]
    
    # Compute new coverage with additional query
    new_coverage = [
        max(current_coverage[j], 
            cosine_similarity(embeddings[new_idx], embeddings[j]))
        for j in range(len(embeddings))
    ]
    
    return sum(new_coverage) - sum(current_coverage)
```

## Practical Benefits

This submodular approach offers several advantages over pure prompting:

1. **Measurable Quality**: Unlike prompting heuristics, we have a concrete objective function to optimize and evaluate
2. **Theoretical Guarantees**: The greedy algorithm provides provable approximation bounds
3. **Scalability**: We can generate many candidates cheaply and select optimally
4. **Tunability**: The parameter $\alpha$ allows systematic control of the relevance-diversity tradeoff
5. **Reproducibility**: Given the same embeddings and $\alpha$, we get deterministic results

## Hyperparameter Tuning

The balance parameter $\alpha$ controls the tradeoff between relevance and diversity:
- **High $\alpha$ (e.g., 0.8)**: Prioritizes relevance to original query, may sacrifice diversity
- **Low $\alpha$ (e.g., 0.2)**: Prioritizes diversity among selected queries, may drift from original intent
- **Moderate $\alpha$ (e.g., 0.4-0.6)**: Balanced approach, often works well in practice

You can tune $\alpha$ using cross-validation on a held-out set or through A/B testing in production systems.

This mathematical framework transforms the intuitive goal of "generating diverse, relevant queries" into a principled optimization problem with clear objectives and guaranteed solution quality.


# Submodular Optimization for Diverse Query Generation

## Understanding Submodularity: The Mathematics of Diminishing Returns

Before trying to solve the problem, let me explain the term submodularity and submodular function to the readers. They may sound unfamiliar to many, but you may well have heard of the idea of "diminishing returns" - well, submodularity is the mathematical representation of that.

Consider placing Wi-Fi routers to provide internet coverage in a large building. The first router you install gives tremendous value - it covers a significant area that previously had no coverage. The second router adds substantial value too, but some of its coverage area overlaps with the first router, so the marginal benefit is less than the first. As you continue adding routers, each additional router covers less and less new area because most spaces are already covered by existing routers. Eventually, the 10th router might provide very little additional coverage since the building is already well-covered.

This intuition captures the essence of submodularity. Mathematically, a set function $f: 2^V \rightarrow \mathbb{R}$ is **submodular** if for all $A \subseteq B \subseteq V$ and any element $v \notin B$:

$f(A \cup \{v\}) - f(A) \geq f(B \cup \{v\}) - f(B)$

In plain English: adding an element to a smaller set gives at least as much benefit as adding the same element to a larger set that contains the smaller set.

## Why Submodular Formulation Matters

You might wonder: why go through the trouble of formulating this as a submodular optimization problem? Why not just use heuristics or other optimization approaches? There are several compelling reasons:

### 1. Theoretical Guarantees
Once we prove our objective function is submodular, we get powerful theoretical guarantees. The greedy algorithm achieves a $(1-1/e) \approx 0.63$ approximation to the optimal solution. This means our greedy solution is always at least 63% as good as the best possible solution. No heuristic can promise this.

### 2. Computational Efficiency  
Finding the optimal subset of $k$ queries from $n$ candidates requires checking $\binom{n}{k}$ combinations - exponential complexity. For just 20 candidates and $k=5$, that's 15,504 combinations. With submodular functions, the greedy algorithm runs in $O(nk)$ time, making it practical even for large candidate sets.

### 3. No Need for Hand-Crafted Heuristics
Without a principled framework, you might resort to ad-hoc rules like "ensure queries have cosine similarity < 0.7" or "balance different keyword categories." These rules are hard to tune and don't generalize. Submodular optimization gives you a principled, mathematically grounded approach.

### 4. Measurable and Comparable Solutions
With a concrete objective function, you can:
- Quantitatively compare different query selection methods
- Tune hyperparameters systematically using validation sets
- Monitor solution quality in production systems
- Debug when the system produces poor results

### 5. Rich Theoretical Foundation
Submodular optimization is a well-studied field with decades of research. You can leverage:
- Advanced algorithms beyond greedy (like accelerated greedy, local search)
- Theoretical insights about when certain formulations work best
- Extensions to handle additional constraints (budget, fairness, etc.)

**Bottom line**: Submodular formulation transforms an ad-hoc "select diverse queries" heuristic into a rigorous optimization problem with provable guarantees, efficient algorithms, and measurable objectives. This matters because query quality directly impacts the effectiveness of your entire DeepSearch system.

## Problem Formulation

Now let's apply this concept to our query generation problem. Given an original input $q_0$, a set of candidate queries $V=\{q_1, q_2, \cdots, q_n\}$ generated by an LLM, we want to select a subset $X\subseteq V$ of $k$ queries that maximizes coverage while minimizing redundancy.

The key insight is that query selection exhibits natural diminishing returns:
- The first query we select covers entirely new semantic space
- The second query should cover different aspects, but some overlap is inevitable  
- As we add more queries, each additional query covers less and less new ground

## Embedding-Based Submodular Function Design

Let $\mathbf{e}_i \in \mathbb{R}^d$ be the embedding vector for query $q_i$, obtained using a sentence embedding model (e.g., jina-embeddings-v3). There are two main approaches to design our objective function:

### Approach 1: Facility Location (Coverage-Based)

$f_{\text{coverage}}(X) = \sum_{j=1}^{n} \max\left(\alpha \cdot \text{sim}(\mathbf{e}_0, \mathbf{e}_j), \max_{q_i \in X} \text{sim}(\mathbf{e}_j, \mathbf{e}_i)\right)$

This function measures how well the selected set $X$ "covers" all candidate queries, where:
- $\text{sim}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{|\mathbf{u}| |\mathbf{v}|}$ is the cosine similarity
- $\alpha \cdot \text{sim}(\mathbf{e}_0, \mathbf{e}_j)$ ensures relevance to the original query
- $\max_{q_i \in X} \text{sim}(\mathbf{e}_j, \mathbf{e}_i)$ measures coverage of candidate $j$ by selected set $X$

**Important caveat**: This function only *implicitly* encourages diversity. It doesn't explicitly penalize similarity within the selected set $X$. Diversity emerges because selecting similar queries provides diminishing coverage returns.

### Approach 2: Explicit Coverage + Diversity

For more direct control over diversity, we can combine coverage and an explicit diversity term:

$f(X) = \lambda \cdot f_{\text{coverage}}(X) + (1-\lambda) \cdot f_{\text{diversity}}(X)$

where the diversity component can be formulated as:

$f_{\text{diversity}}(X) = \sum_{q_i \in X} \sum_{q_j \in V \setminus X} \text{sim}(\mathbf{e}_i, \mathbf{e}_j)$

This diversity term measures the total similarity between selected queries and unselected queries - it's maximized when we select queries that are different from the remaining candidates (a form of graph cut function).

**Submodularity check**: Both formulations maintain submodularity. The facility location function is a well-known submodular function, and the graph cut diversity term is also submodular when measuring connections between selected and unselected sets.

## Why These Functions are Submodular

**Facility Location Function**: Exhibits submodularity because of the max operation. When we add a new query $q$ to our selected set, each candidate query $j$ gets covered by the "best" query in our set (the one with highest similarity). Adding $q$ to a smaller set $A$ is more likely to improve the coverage of various candidates than adding it to a larger set $B \supseteq A$ where many candidates are already well-covered.

**Graph Cut Diversity Function**: The diversity term $\sum_{q_i \in X} \sum_{q_j \in V \setminus X} \text{sim}(\mathbf{e}_i, \mathbf{e}_j)$ is submodular because it measures the "cut" between selected and unselected sets. Adding a new query to a smaller selected set creates more new connections to unselected queries than adding it to a larger selected set.

**Key insight**: The facility location approach relies on *implicit* diversity through coverage competition, while the explicit approach directly measures and optimizes for diversity. Both are valid, but the explicit approach gives you more direct control over the relevance-diversity tradeoff.

## Intuitive Understanding

Let me illustrate both approaches with a concrete example. Suppose our original query is "machine learning optimization" and we're selecting from these candidates:

1. "gradient descent algorithms"  
2. "neural network training"
3. "stochastic optimization methods" 
4. "backpropagation techniques"
5. "optimization for deep learning"

**Facility Location (Implicit Diversity)**:
- **First selection**: "gradient descent algorithms" covers fundamental optimization concepts
- **Second selection**: "neural network training" covers application domain with some overlap
- **Third selection**: "backpropagation techniques" provides less marginal coverage benefit since it's mostly in the intersection of optimization + neural networks

The diversity emerges naturally because similar queries compete to cover the same semantic regions.

**Explicit Coverage + Diversity**:
- The coverage term works as above
- The diversity term explicitly rewards selecting "stochastic optimization methods" over "backpropagation techniques" because it has higher similarity to unselected candidates (indicating it covers different semantic space)

**Which approach to use?**
- **Facility location**: Simpler, well-studied, good default choice
- **Explicit diversity**: More control over relevance-diversity balance, better when you need guaranteed diversity within selected set

You're absolutely right to point out that the facility location approach doesn't directly measure diversity *within* the selected queries - it only ensures good coverage of the candidate space. The explicit approach gives you that direct control.

## Optimization Algorithm

Since our function is submodular, we can use the greedy algorithm which provides a $(1-1/e) \approx 0.63$ approximation guarantee:

$$\max_{X \subseteq V} f(X) \quad \text{subject to} \quad |X| \leq k$$

```python
def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):
    """
    Greedy algorithm for submodular query selection
    
    Args:
        candidates: List of candidate query strings
        embeddings: Matrix of query embeddings (n x d)
        original_embedding: Embedding of original query (d,)
        k: Number of queries to select
        alpha: Relevance weight parameter
    """
    n = len(candidates)
    selected = []
    remaining = set(range(n))
    
    # Precompute relevance scores
    relevance_scores = cosine_similarity(original_embedding, embeddings)
    
    for _ in range(k):
        best_gain = -float('inf')
        best_query = None
        
        for i in remaining:
            # Calculate marginal gain of adding query i
            gain = compute_marginal_gain(i, selected, embeddings, 
                                       relevance_scores, alpha)
            if gain > best_gain:
                best_gain = gain
                best_query = i
        
        if best_query is not None:
            selected.append(best_query)
            remaining.remove(best_query)
    
    return [candidates[i] for i in selected]

def compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):
    """Compute marginal gain of adding new_idx to selected set"""
    if not selected:
        # First query: gain is sum of all relevance scores
        return sum(max(alpha * relevance_scores[j], 
                      cosine_similarity(embeddings[new_idx], embeddings[j]))
                  for j in range(len(embeddings)))
    
    # Compute current coverage
    current_coverage = [
        max([alpha * relevance_scores[j]] + 
            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])
        for j in range(len(embeddings))
    ]
    
    # Compute new coverage with additional query
    new_coverage = [
        max(current_coverage[j], 
            cosine_similarity(embeddings[new_idx], embeddings[j]))
        for j in range(len(embeddings))
    ]
    
    return sum(new_coverage) - sum(current_coverage)
```

## Practical Benefits

This submodular approach offers several advantages over pure prompting:

1. **Measurable Quality**: Unlike prompting heuristics, we have a concrete objective function to optimize and evaluate
2. **Theoretical Guarantees**: The greedy algorithm provides provable approximation bounds
3. **Scalability**: We can generate many candidates cheaply and select optimally
4. **Tunability**: The parameter $\alpha$ allows systematic control of the relevance-diversity tradeoff
5. **Reproducibility**: Given the same embeddings and $\alpha$, we get deterministic results

## Hyperparameter Tuning

The balance parameter $\alpha$ controls the tradeoff between relevance and diversity:
- **High $\alpha$ (e.g., 0.8)**: Prioritizes relevance to original query, may sacrifice diversity
- **Low $\alpha$ (e.g., 0.2)**: Prioritizes diversity among selected queries, may drift from original intent
- **Moderate $\alpha$ (e.g., 0.4-0.6)**: Balanced approach, often works well in practice

You can tune $\alpha$ using cross-validation on a held-out set or through A/B testing in production systems.

This mathematical framework transforms the intuitive goal of "generating diverse, relevant queries" into a principled optimization problem with clear objectives and guaranteed solution quality.
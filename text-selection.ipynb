{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "If you are interested in learning more about the applications of submodularity/diminishing returns, check out the blog post here https://jina.ai/news/submodular-optimization-for-diverse-query-generation-in-deepresearch"
      ],
      "metadata": {
        "id": "icRQBtiR-kEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from transformers import AutoModel, AutoProcessor, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "\n",
        "# Initialize the model (this may take a moment)\n",
        "print(\"Loading model...\")\n",
        "model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v4\", trust_remote_code=True, torch_dtype=torch.float16)\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "preprocessor = AutoProcessor.from_pretrained('jinaai/jina-embeddings-v4', trust_remote_code=True)\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "FFVXfk624mL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_marginal_gain_diversity(new_idx, selected, embeddings, similarity_matrix):\n",
        "    n = similarity_matrix.shape[0]\n",
        "\n",
        "    if not selected:\n",
        "        return np.sum(similarity_matrix[new_idx])\n",
        "\n",
        "    # Vectorized computation of current coverage\n",
        "    current_coverage = np.max(similarity_matrix[selected], axis=0)\n",
        "\n",
        "    # Compute new coverage and marginal gain\n",
        "    new_coverage = np.maximum(current_coverage, similarity_matrix[new_idx])\n",
        "    return np.sum(new_coverage - current_coverage)\n",
        "\n",
        "\n",
        "def lazy_greedy_token_selection(embeddings, k):\n",
        "    n = len(embeddings)\n",
        "    selected = []\n",
        "    remaining = set(range(n))\n",
        "\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    pq = []\n",
        "    for i in range(n):\n",
        "        gain = compute_marginal_gain_diversity(i, [], embeddings, similarity_matrix)\n",
        "        heapq.heappush(pq, (-gain, 0, i))\n",
        "\n",
        "    for iteration in range(k):\n",
        "        while pq:\n",
        "            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n",
        "\n",
        "            if best_idx not in remaining:\n",
        "                continue\n",
        "\n",
        "            if last_updated == iteration:\n",
        "                selected.append(best_idx)\n",
        "                remaining.remove(best_idx)\n",
        "                break\n",
        "\n",
        "            current_gain = compute_marginal_gain_diversity(best_idx, selected, embeddings, similarity_matrix)\n",
        "            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n",
        "\n",
        "    return selected\n",
        "\n",
        "# Global variables to store processed data\n",
        "current_embeddings = None\n",
        "current_input_ids = None\n",
        "current_text = \"\"\n",
        "current_similarity_matrix = None  # Cache the similarity matrix too\n",
        "current_token_strings = None  # Cache the token strings too\n",
        "current_sentences = None  # Cache the sentences for sentence mode\n",
        "\n",
        "def split_by_punctuation(text, punctuation_chars):\n",
        "    \"\"\"Split text by punctuation characters, keeping the punctuation with the sentences\"\"\"\n",
        "    import re\n",
        "    # Escape special regex characters for the lookbehind\n",
        "    escaped_chars = re.escape(punctuation_chars)\n",
        "    # Use positive lookbehind to split after punctuation while keeping it\n",
        "    sentences = re.split(f'(?<=[{escaped_chars}])', text)\n",
        "    # Filter out empty sentences and strip whitespace\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"Process the input text and generate embeddings\"\"\"\n",
        "    global current_embeddings, current_input_ids, current_text, current_similarity_matrix, current_token_strings, current_sentences\n",
        "\n",
        "    if not text.strip():\n",
        "        return None, None\n",
        "\n",
        "    # Only recompute if text has actually changed\n",
        "    if current_text == text.strip():\n",
        "        return current_embeddings, current_input_ids\n",
        "\n",
        "    current_text = text.strip()\n",
        "\n",
        "    if sentence_mode_toggle.value:\n",
        "        # Sentence mode\n",
        "        sentences = split_by_punctuation(current_text, punctuation_input.value)\n",
        "        current_sentences = sentences\n",
        "\n",
        "        if not sentences:\n",
        "            return None, None\n",
        "\n",
        "        # Generate sentence-level embeddings\n",
        "        mv_embed = model.encode_text(\n",
        "            texts=sentences,\n",
        "            task=\"text-matching\",\n",
        "            prompt_name=\"query\",\n",
        "            return_numpy=True\n",
        "        )\n",
        "\n",
        "        current_embeddings = mv_embed\n",
        "        current_input_ids = None  # Not used in sentence mode\n",
        "        current_token_strings = sentences  # Use sentences as \"tokens\" for display\n",
        "\n",
        "    else:\n",
        "        # Token mode (original behavior)\n",
        "        # Generate multi-vector embeddings\n",
        "        mv_embed = model.encode_text(\n",
        "            texts=[current_text],\n",
        "            task=\"text-matching\",\n",
        "            return_multivector=True,\n",
        "            prompt_name=\"query\"\n",
        "        )[0][2:]  # Skip first 2 tokens\n",
        "\n",
        "        # Get input IDs\n",
        "        preprocessor_results = preprocessor.process_texts(\n",
        "            texts=[current_text],\n",
        "            prefix=\"query\"\n",
        "        )\n",
        "        input_ids = preprocessor_results[\"input_ids\"][0].tolist()[2:]  # Skip first 2 tokens\n",
        "\n",
        "        current_embeddings = mv_embed.cpu().numpy()\n",
        "        current_input_ids = input_ids\n",
        "        current_sentences = None\n",
        "\n",
        "        # Pre-compute token strings to avoid repeated tokenizer calls\n",
        "        current_token_strings = preprocessor.tokenizer.convert_ids_to_tokens(current_input_ids)\n",
        "\n",
        "    # Pre-compute similarity matrix since it's expensive and only depends on embeddings\n",
        "    current_similarity_matrix = cosine_similarity(current_embeddings)\n",
        "\n",
        "    return current_embeddings, current_input_ids\n",
        "\n",
        "def update_display(k_value):\n",
        "    \"\"\"Update the display with selected tokens or sentences\"\"\"\n",
        "    if current_embeddings is None:\n",
        "        return\n",
        "\n",
        "    # Ensure k_value doesn't exceed available items\n",
        "    max_items = len(current_sentences) if sentence_mode_toggle.value else len(current_input_ids) if current_input_ids else 0\n",
        "    k_value = min(k_value, max_items)\n",
        "\n",
        "    if k_value <= 0:\n",
        "        combined_text_output.value = \"\"\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            print(\"❌ No items available for selection\")\n",
        "        return\n",
        "\n",
        "    # Select diverse items using cached similarity matrix\n",
        "    selected_indices = lazy_greedy_token_selection_cached(current_embeddings, k_value)\n",
        "    selected_indices_sorted = sorted(selected_indices)\n",
        "\n",
        "    if sentence_mode_toggle.value:\n",
        "        # Sentence mode - add bounds checking\n",
        "        valid_indices = [i for i in selected_indices_sorted if i < len(current_sentences)]\n",
        "        selected_sentences = [current_sentences[i] for i in valid_indices]\n",
        "        combined_text = ' '.join(selected_sentences)\n",
        "\n",
        "        # Update debug output\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            print(f\"📊 Selected {len(valid_indices)} out of {len(current_sentences)} sentences\")\n",
        "            print(f\"📝 Selected sentences:\")\n",
        "            for i, sentence in enumerate(selected_sentences):\n",
        "                print(f\"  {i+1}. {sentence}\")\n",
        "            print(f\"📍 Sentence positions: {valid_indices}\")\n",
        "    else:\n",
        "        # Token mode (original behavior) - add bounds checking\n",
        "        if current_input_ids is None:\n",
        "            return\n",
        "        valid_indices = [i for i in selected_indices_sorted if i < len(current_input_ids)]\n",
        "        selected_token_ids = [current_input_ids[i] for i in valid_indices]\n",
        "        selected_strings = [current_token_strings[i] for i in valid_indices]\n",
        "        combined_text = preprocessor.tokenizer.convert_tokens_to_string(selected_strings)\n",
        "\n",
        "        # Update debug output\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            print(f\"📊 Selected {len(valid_indices)} out of {len(current_input_ids)} tokens\")\n",
        "            print(f\"🔤 Selected tokens: {selected_strings}\")\n",
        "            print(f\"📍 Token positions: {valid_indices}\")\n",
        "\n",
        "    # Update the combined text output box\n",
        "    combined_text_output.value = combined_text\n",
        "\n",
        "def lazy_greedy_token_selection_cached(embeddings, k):\n",
        "    \"\"\"Optimized version that uses pre-computed similarity matrix\"\"\"\n",
        "    n = len(embeddings)\n",
        "    selected = []\n",
        "    remaining = set(range(n))\n",
        "\n",
        "    # Use cached similarity matrix\n",
        "    similarity_matrix = current_similarity_matrix\n",
        "\n",
        "    pq = []\n",
        "    for i in range(n):\n",
        "        gain = compute_marginal_gain_diversity(i, [], embeddings, similarity_matrix)\n",
        "        heapq.heappush(pq, (-gain, 0, i))\n",
        "\n",
        "    for iteration in range(k):\n",
        "        while pq:\n",
        "            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n",
        "\n",
        "            if best_idx not in remaining:\n",
        "                continue\n",
        "\n",
        "            if last_updated == iteration:\n",
        "                selected.append(best_idx)\n",
        "                remaining.remove(best_idx)\n",
        "                break\n",
        "\n",
        "            current_gain = compute_marginal_gain_diversity(best_idx, selected, embeddings, similarity_matrix)\n",
        "            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n",
        "\n",
        "    return selected\n",
        "\n",
        "def on_text_change(change):\n",
        "    \"\"\"Handle text input changes\"\"\"\n",
        "    text = change['new']\n",
        "    if text.strip():\n",
        "        try:\n",
        "            process_text(text)\n",
        "            # Update slider max value\n",
        "            max_items = len(current_sentences) if sentence_mode_toggle.value else len(current_input_ids) if current_input_ids else 0\n",
        "            if max_items > 0:\n",
        "                k_slider.max = max_items\n",
        "                k_slider.value = min(k_slider.value, max_items)\n",
        "                update_display(k_slider.value)\n",
        "        except Exception as e:\n",
        "            with output_area:\n",
        "                clear_output()\n",
        "                print(f\"❌ Error processing text: {str(e)}\")\n",
        "\n",
        "def on_slider_change(change):\n",
        "    \"\"\"Handle slider value changes - now much faster!\"\"\"\n",
        "    if current_embeddings is not None:\n",
        "        update_display(change['new'])\n",
        "\n",
        "def on_mode_change(change):\n",
        "    \"\"\"Handle mode toggle changes\"\"\"\n",
        "    global current_text\n",
        "    if current_text:\n",
        "        try:\n",
        "            # Force reprocessing by clearing current_text\n",
        "            temp_text = current_text\n",
        "            current_text = \"\"\n",
        "            process_text(temp_text)\n",
        "\n",
        "            # Update slider max value and constrain current value\n",
        "            max_items = len(current_sentences) if sentence_mode_toggle.value else len(current_input_ids) if current_input_ids else 0\n",
        "            if max_items > 0:\n",
        "                k_slider.max = max_items\n",
        "                k_slider.value = min(k_slider.value, max_items)\n",
        "                # Force update display with the new constrained value\n",
        "                update_display(k_slider.value)\n",
        "            else:\n",
        "                with output_area:\n",
        "                    clear_output()\n",
        "                    print(\"❌ No items found after mode switch\")\n",
        "        except Exception as e:\n",
        "            with output_area:\n",
        "                clear_output()\n",
        "                print(f\"❌ Error switching mode: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "def on_punctuation_change(change):\n",
        "    \"\"\"Handle punctuation input changes\"\"\"\n",
        "    if sentence_mode_toggle.value and current_text:\n",
        "        on_mode_change(None)  # Reprocess text with new punctuation\n",
        "\n",
        "# Create UI components\n",
        "text_input = widgets.Textarea(\n",
        "    value=\"\"\"Founded in 2020, Jina AI is a leading search AI company. Our Search Foundation platform combines Embeddings, Rerankers, and Small Language Models to help businesses build reliable and high-quality GenAI and multimodal search applications.\"\"\",\n",
        "    placeholder='Enter your text here...',\n",
        "    description='Input Text:',\n",
        "    layout=widgets.Layout(width='100%', height='120px')\n",
        ")\n",
        "\n",
        "# New UI components for sentence mode\n",
        "sentence_mode_toggle = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Sentence Selection Mode',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "punctuation_input = widgets.Text(\n",
        "    value=',.!?，。！？',\n",
        "    placeholder='Punctuation characters for splitting...',\n",
        "    description='Punctuation:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "k_slider = widgets.IntSlider(\n",
        "    value=30,\n",
        "    min=1,\n",
        "    max=100,\n",
        "    step=1,\n",
        "    description='Items (k):',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "\n",
        "# Add a separate text area for the combined text output\n",
        "combined_text_output = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Selected text will appear here...',\n",
        "    description='Selected:',\n",
        "    layout=widgets.Layout(width='100%', height='120px'),\n",
        "    disabled=False  # Allow users to copy/edit the text\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Connect event handlers\n",
        "text_input.observe(on_text_change, names='value')\n",
        "k_slider.observe(on_slider_change, names='value')\n",
        "sentence_mode_toggle.observe(on_mode_change, names='value')\n",
        "punctuation_input.observe(on_punctuation_change, names='value')\n",
        "\n",
        "# Create the UI layout\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h2>Submodular Optimization for Token/Sentence Selection</h2>\"),\n",
        "    widgets.HTML(\"<p>Enter text below and use the controls to select tokens or sentences:</p>\"),\n",
        "    text_input,\n",
        "    widgets.HBox([sentence_mode_toggle, punctuation_input]),\n",
        "    k_slider,\n",
        "    combined_text_output,\n",
        "    widgets.HTML(\"<h3>📋 Debug Info:</h3>\"),\n",
        "    output_area\n",
        "])\n",
        "\n",
        "# Process initial text\n",
        "if text_input.value.strip():\n",
        "    process_text(text_input.value)\n",
        "    max_items = len(current_sentences) if sentence_mode_toggle.value else len(current_input_ids) if current_input_ids else 0\n",
        "    if max_items > 0:\n",
        "        k_slider.max = max_items\n",
        "        k_slider.value = min(k_slider.value, max_items)\n",
        "        update_display(k_slider.value)\n",
        "\n",
        "# Display the UI\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "UK1lLkhk5XeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z937DFBujY5V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}